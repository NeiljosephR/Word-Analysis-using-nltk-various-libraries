# -*- coding: utf-8 -*-
"""Untitled69.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Nx41OauP_P-nd68gVsbcXu2VZgh0FI88
"""

import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize
text="""Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome. The sky is pinkish-blue. You shouldn't eat cardboard"""

tokenized_text=sent_tokenize(text)
print(tokenized_text)

from nltk.tokenize import word_tokenize

tokenized_word=word_tokenize(text)

print(tokenized_word)

from nltk.probability import FreqDist

fdist = FreqDist(tokenized_word)
print(fdist.most_common(3))

import matplotlib.pyplot as plt

fdist.plot(30,cumulative=False)
plt.show()

from nltk.corpus import stopwords

nltk.download("stopwords")

stop_words=list(stopwords.words("english"))

print(stop_words)
for i in stop_words:
  if i=="how":
    print(i) 
len(stop_words)

filtered_tokens=[]
for w in tokenized_word:
  if w not in stop_words:
    filtered_tokens.append(w)

print("Tokenized Words:",tokenized_word)
print("Filtered Tokens:",filtered_tokens)

import string

# punctuations
punctuations=list(string.punctuation)

filtered_tokens2=[]

for i in filtered_tokens:
    if i not in punctuations:
        filtered_tokens2.append(i)

print("Filtered Tokens After Removing Punctuations:",filtered_tokens2)

from nltk.stem import PorterStemmer
from nltk.tokenize import sent_tokenize, word_tokenize
ps = PorterStemmer()

stemmed_words=[]

for w in filtered_tokens2:
  stemmed_words.append(ps.stem(w))

print("Filtered Tokens After Removing Punctuations:",filtered_tokens2)
print("Stemmed Tokens:",stemmed_words)

nltk.download("wordnet")
nltk.download('omw-1.4')
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer


lem = WordNetLemmatizer()
stem = PorterStemmer()
word = "flying"

print("Lemmatized Word:",lem.lemmatize(word,"v"))

print("Stemmed Word:",stem.stem(word))

nltk.download('averaged_perceptron_tagger')
from nltk.tokenize import word_tokenize
from nltk import pos_tag

sent = "Albert Einstein was born in Ulm, Germany in 1879."

tokens=word_tokenize(sent)
pos_=pos_tag(tokens)


print("Tokens:",tokens)
print("PoS tags:",pos_)

import nltk
from nltk import ne_chunk
nltk.download('maxent_ne_chunker')
nltk.download("words")

sent="Cruz IS FROM Hyderabad"
for chunk in ne_chunk(nltk.pos_tag(word_tokenize(sent))):
  if hasattr(chunk, 'label'):
    print(chunk.label(), ' '.join(c[0] for c in chunk))